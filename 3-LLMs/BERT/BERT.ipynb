{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='nlptown/bert-base-multilingual-uncased-sentiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting TensorFlow\n",
      "  Using cached tensorflow-2.13.1-cp38-cp38-win_amd64.whl.metadata (2.6 kB)\n",
      "INFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached tensorflow-2.13.0-cp38-cp38-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting tensorflow-intel==2.13.0 (from TensorFlow)\n",
      "  Using cached tensorflow_intel-2.13.0-cp38-cp38-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.13.0->TensorFlow)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.13.0->TensorFlow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from tensorflow-intel==2.13.0->TensorFlow) (24.3.25)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-intel==2.13.0->TensorFlow)\n",
      "  Using cached gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.13.0->TensorFlow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=2.9.0 (from tensorflow-intel==2.13.0->TensorFlow)\n",
      "  Using cached h5py-3.11.0-cp38-cp38-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.13.0->TensorFlow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting numpy<=1.24.3,>=1.22 (from tensorflow-intel==2.13.0->TensorFlow)\n",
      "  Using cached numpy-1.24.3-cp38-cp38-win_amd64.whl.metadata (5.6 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.13.0->TensorFlow)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from tensorflow-intel==2.13.0->TensorFlow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from tensorflow-intel==2.13.0->TensorFlow) (4.25.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from tensorflow-intel==2.13.0->TensorFlow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from tensorflow-intel==2.13.0->TensorFlow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.13.0->TensorFlow)\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow-intel==2.13.0->TensorFlow)\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from tensorflow-intel==2.13.0->TensorFlow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from tensorflow-intel==2.13.0->TensorFlow) (1.64.0)\n",
      "Collecting tensorboard<2.14,>=2.13 (from tensorflow-intel==2.13.0->TensorFlow)\n",
      "  Using cached tensorboard-2.13.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow-intel==2.13.0->TensorFlow)\n",
      "  Using cached tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.14,>=2.13.1 (from tensorflow-intel==2.13.0->TensorFlow)\n",
      "  Using cached keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.13.0->TensorFlow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp38-cp38-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->TensorFlow) (0.43.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->TensorFlow) (2.29.0)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->TensorFlow)\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->TensorFlow)\n",
      "  Using cached Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->TensorFlow) (2.31.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->TensorFlow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->TensorFlow) (3.0.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->TensorFlow) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->TensorFlow) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->TensorFlow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->TensorFlow) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->TensorFlow) (7.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->TensorFlow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->TensorFlow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->TensorFlow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->TensorFlow) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->TensorFlow) (2.1.5)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->TensorFlow) (3.17.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->TensorFlow) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->TensorFlow) (3.2.2)\n",
      "Using cached tensorflow-2.13.0-cp38-cp38-win_amd64.whl (1.9 kB)\n",
      "Using cached tensorflow_intel-2.13.0-cp38-cp38-win_amd64.whl (276.5 MB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached h5py-3.11.0-cp38-cp38-win_amd64.whl (3.0 MB)\n",
      "Using cached keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Using cached numpy-1.24.3-cp38-cp38-win_amd64.whl (14.9 MB)\n",
      "Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Using cached tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "Using cached tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.31.0-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Using cached Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Installing collected packages: libclang, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, numpy, keras, google-pasta, gast, astunparse, absl-py, opt-einsum, markdown, h5py, google-auth-oauthlib, tensorboard, tensorflow-intel, TensorFlow\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.4\n",
      "    Uninstalling numpy-1.24.4:\n",
      "      Successfully uninstalled numpy-1.24.4\n",
      "Successfully installed TensorFlow-2.13.0 absl-py-2.1.0 astunparse-1.6.3 gast-0.4.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 h5py-3.11.0 keras-2.13.1 libclang-18.1.1 markdown-3.6 numpy-1.24.3 opt-einsum-3.3.0 tensorboard-2.13.0 tensorboard-data-server-0.7.2 tensorflow-estimator-2.13.0 tensorflow-intel-2.13.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.4.0 typing-extensions-4.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Ziad\\anaconda3\\envs\\Genai\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sqlalchemy 2.0.30 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "emoji 2.12.1 requires typing-extensions>=4.7.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "fastapi 0.111.0 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "groq 0.9.0 requires typing-extensions<5,>=4.7, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "pydantic-core 2.18.2 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "selenium 4.22.0 requires typing_extensions>=4.9.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "torch 2.3.0 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "unstructured-client 0.22.0 requires dataclasses-json>=0.6.4, but you have dataclasses-json 0.5.14 which is incompatible.\n",
      "unstructured-client 0.22.0 requires typing-extensions>=4.7.1, but you have typing-extensions 4.5.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install TensorFlow \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ziad\\anaconda3\\envs\\Genai\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2c4439c2ec424980e37dc7fb57fb3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tf_model.h5:   0%|          | 0.00/670M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ziad\\anaconda3\\envs\\Genai\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Ziad\\.cache\\huggingface\\hub\\models--nlptown--bert-base-multilingual-uncased-sentiment. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at nlptown/bert-base-multilingual-uncased-sentiment.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer,TFBertForSequenceClassification\n",
    "\n",
    "tokenizer=BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "mocel=TFBertForSequenceClassification.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 151, 11157, 11153, 12013, 10373, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n",
      "1/1 [==============================] - 0s 95ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='i love my jop'\n",
    "\n",
    "inputs=tokenizer(text)\n",
    "print(inputs)\n",
    "\n",
    "input_ids=inputs['input_ids']\n",
    "\n",
    "predictions=mocel.predict([input_ids])\n",
    "\n",
    "logits=predictions.logits\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "predicted_class=np.argmax(logits)\n",
    "\n",
    "predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the text is predicted to have a sentiment :very positive\n"
     ]
    }
   ],
   "source": [
    "sentiment_mapping={\n",
    "    0:'very negative',\n",
    "    1:' negative',\n",
    "    2:'neutral',\n",
    "    3:'positive',\n",
    "    4:'very positive',\n",
    "}\n",
    "\n",
    "predicted_sentiment=sentiment_mapping[predicted_class]\n",
    "\n",
    "print('the text is predicted to have a sentiment :{}'.format(predicted_sentiment))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'return_tensores': 'tf'} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 151ms/step\n",
      "the text is predicted to have a sentiment :very negative\n"
     ]
    }
   ],
   "source": [
    "text='i hate my jop'\n",
    "\n",
    "inputs=tokenizer(text,return_tensores='tf')\n",
    "\n",
    "input_ids=inputs['input_ids']\n",
    "\n",
    "predictions=mocel.predict([input_ids])\n",
    "\n",
    "logits=predictions.logits\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "predicted_class=np.argmax(logits)\n",
    "predicted_class\n",
    "\n",
    "predicted_sentiment=sentiment_mapping[predicted_class]\n",
    "\n",
    "print('the text is predicted to have a sentiment :{}'.format(predicted_sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#useing BERT to make sentiment reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('IMDB Dataset.csv')\n",
    "\n",
    "\n",
    "first_5_rows=df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 1s 580ms/step\n",
      "1/1 [==============================] - 1s 510ms/step\n",
      "1/1 [==============================] - 1s 828ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ziad\\AppData\\Local\\Temp\\ipykernel_17108\\1289842158.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  first_5_rows['predicted_sentiment'] = predicted_sentiments\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>predicted_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>very positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production. <br /><br />The...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "\n",
       "  predicted_sentiment  \n",
       "0             neutral  \n",
       "1       very positive  \n",
       "2            positive  \n",
       "3             neutral  \n",
       "4            positive  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Mapping for sentiment classes\n",
    "sentiment_mapping = {\n",
    "    0: 'very negative',\n",
    "    1: 'negative',\n",
    "    2: 'neutral',\n",
    "    3: 'positive',\n",
    "    4: 'very positive',\n",
    "}\n",
    "\n",
    "predicted_sentiments = []\n",
    "\n",
    "# Loop through the reviews\n",
    "for title in first_5_rows['review']:\n",
    "    # Tokenize the text\n",
    "    inputs = tokenizer(title, return_tensors='tf')\n",
    "\n",
    "    input_ids = inputs['input_ids']\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = mocel.predict([input_ids])\n",
    "\n",
    "    logits = predictions.logits\n",
    "\n",
    "    # Determine the predicted class\n",
    "    predicted_class = np.argmax(logits)\n",
    "\n",
    "    # Map the class to the sentiment\n",
    "    predicted_sentiment = sentiment_mapping[predicted_class]\n",
    "\n",
    "    # Append the sentiment to the list\n",
    "    predicted_sentiments.append(predicted_sentiment)\n",
    "\n",
    "# Add the predicted sentiments to the dataframe\n",
    "first_5_rows['predicted_sentiment'] = predicted_sentiments\n",
    "\n",
    "# Display the dataframe\n",
    "first_5_rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#sentiment Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting camel-tools\n",
      "  Downloading camel_tools-1.5.2-py3-none-any.whl.metadata (9.6 kB)\n",
      "Collecting future (from camel-tools)\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: six in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from camel-tools) (1.16.0)\n",
      "Collecting docopt (from camel-tools)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: cachetools in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from camel-tools) (5.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from camel-tools) (1.24.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from camel-tools) (1.10.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from camel-tools) (2.0.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from camel-tools) (1.3.2)\n",
      "Collecting dill (from camel-tools)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch>=1.3 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from camel-tools) (2.3.0)\n",
      "Requirement already satisfied: transformers>=3.0.2 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from camel-tools) (4.40.2)\n",
      "Collecting editdistance (from camel-tools)\n",
      "  Downloading editdistance-0.8.1-cp38-cp38-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from camel-tools) (2.31.0)\n",
      "Requirement already satisfied: emoji in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from camel-tools) (2.12.1)\n",
      "Collecting pyrsistent (from camel-tools)\n",
      "  Downloading pyrsistent-0.20.0-cp38-cp38-win_amd64.whl.metadata (976 bytes)\n",
      "Requirement already satisfied: tabulate in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from camel-tools) (0.9.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from camel-tools) (4.66.4)\n",
      "Collecting muddler (from camel-tools)\n",
      "  Downloading muddler-0.1.3-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from torch>=1.3->camel-tools) (3.14.0)\n",
      "Collecting typing-extensions>=4.8.0 (from torch>=1.3->camel-tools)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: sympy in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from torch>=1.3->camel-tools) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from torch>=1.3->camel-tools) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from torch>=1.3->camel-tools) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from torch>=1.3->camel-tools) (2024.5.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from torch>=1.3->camel-tools) (2021.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (0.23.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (0.4.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from tqdm->camel-tools) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from pandas->camel-tools) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from pandas->camel-tools) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from pandas->camel-tools) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from requests->camel-tools) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from requests->camel-tools) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from requests->camel-tools) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from requests->camel-tools) (2024.2.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from scikit-learn->camel-tools) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from scikit-learn->camel-tools) (3.5.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.3->camel-tools) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.3->camel-tools) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from jinja2->torch>=1.3->camel-tools) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\ziad\\anaconda3\\envs\\genai\\lib\\site-packages (from sympy->torch>=1.3->camel-tools) (1.3.0)\n",
      "Downloading camel_tools-1.5.2-py3-none-any.whl (124 kB)\n",
      "   ---------------------------------------- 0.0/124.3 kB ? eta -:--:--\n",
      "   ------------------- -------------------- 61.4/124.3 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 124.3/124.3 kB 1.0 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "   ---------------------------------------- 0.0/116.3 kB ? eta -:--:--\n",
      "   ------------------------------- -------- 92.2/116.3 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 116.3/116.3 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading editdistance-0.8.1-cp38-cp38-win_amd64.whl (79 kB)\n",
      "   ---------------------------------------- 0.0/79.6 kB ? eta -:--:--\n",
      "   ------------------------------ --------- 61.4/79.6 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 79.6/79.6 kB 1.5 MB/s eta 0:00:00\n",
      "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "   ---------------------------------------- 0.0/491.3 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 51.2/491.3 kB 1.3 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 102.4/491.3 kB 1.2 MB/s eta 0:00:01\n",
      "   --------- ---------------------------- 122.9/491.3 kB 901.1 kB/s eta 0:00:01\n",
      "   ------------- ------------------------ 174.1/491.3 kB 952.6 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 204.8/491.3 kB 888.4 kB/s eta 0:00:01\n",
      "   ------------------ ------------------- 235.5/491.3 kB 846.9 kB/s eta 0:00:01\n",
      "   -------------------- ----------------- 266.2/491.3 kB 817.9 kB/s eta 0:00:01\n",
      "   ---------------------- --------------- 286.7/491.3 kB 803.7 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 317.4/491.3 kB 785.7 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 348.2/491.3 kB 746.0 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 378.9/491.3 kB 737.3 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 430.1/491.3 kB 726.4 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 460.8/491.3 kB 720.9 kB/s eta 0:00:01\n",
      "   -------------------------------------- 491.3/491.3 kB 715.9 kB/s eta 0:00:00\n",
      "Downloading muddler-0.1.3-py3-none-any.whl (16 kB)\n",
      "Downloading pyrsistent-0.20.0-cp38-cp38-win_amd64.whl (63 kB)\n",
      "   ---------------------------------------- 0.0/63.3 kB ? eta -:--:--\n",
      "   ------------------------- -------------- 41.0/63.3 kB 960.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 63.3/63.3 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Building wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13773 sha256=dc35b44456d1895aca798f32e0e56984a6b0ee7673e8dcf9fe2dc49283840227\n",
      "  Stored in directory: c:\\users\\ziad\\appdata\\local\\pip\\cache\\wheels\\56\\ea\\58\\ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
      "Successfully built docopt\n",
      "Installing collected packages: docopt, typing-extensions, pyrsistent, muddler, future, editdistance, dill, camel-tools\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "Successfully installed camel-tools-1.5.2 dill-0.3.8 docopt-0.6.2 editdistance-0.8.1 future-1.0.0 muddler-0.1.3 pyrsistent-0.20.0 typing-extensions-4.12.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.12.2 which is incompatible.\n",
      "unstructured-client 0.22.0 requires dataclasses-json>=0.6.4, but you have dataclasses-json 0.5.14 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install camel-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ziad\\anaconda3\\envs\\Genai\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8ec379d68e48a78d26dd44cd9d39c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   7%|7         | 31.5M/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b63ccc00a646c7b45fe962ba616172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/86.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4549e08c83cc40959e8673770382f18f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/305k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc01090600e6452393f21d5bc4785a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['positive']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from camel_tools.sentiment import SentimentAnalyzer\n",
    "\n",
    "model_name=SentimentAnalyzer('CAMeL-Lab/bert-base-arabic-camelbert-da-sentiment')\n",
    "\n",
    "sentince='أنا أعشق عملي'\n",
    "\n",
    "model_name.predict(sentince)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                tweet class  \\\n",
      "0   ' #علمتني_الحياه أن الذين يعيشون على الأرض ليس...   pos   \n",
      "1   ' #ميري_كرسمس كل سنة وانتم طيبين http://t.co/n...   pos   \n",
      "2                           ' و انتهى مشوار الخواجة '   neg   \n",
      "3                    ' مش عارف ابتدى مذاكره منين :/ '   neg   \n",
      "4   ' @mskhafagi  إختصروا الطريق بدلا من إختيار ال...   neg   \n",
      "..                                                ...   ...   \n",
      "95  ' @ayamohamed9926 هتتحاسبى على اى استخدام خارج...   neu   \n",
      "96       ' حتى الحشيش بيقلب المواجع و بقى يطلب بنكد '   neg   \n",
      "97  ' @SaddamHkhatib  هههههههههههههههههههههههههههه...   pos   \n",
      "98                       ' صوتها بيوهب قلبي حياة ♪♬ '   pos   \n",
      "99                     ' @Marina_maher_ انا بمتحن ✋ '   neu   \n",
      "\n",
      "   predicted_sentiment  \n",
      "0             negative  \n",
      "1             positive  \n",
      "2             negative  \n",
      "3             negative  \n",
      "4             negative  \n",
      "..                 ...  \n",
      "95             neutral  \n",
      "96            negative  \n",
      "97            positive  \n",
      "98            positive  \n",
      "99             neutral  \n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ziad\\AppData\\Local\\Temp\\ipykernel_17108\\3740823714.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  first_100_rows['predicted_sentiment'] = predicted_sentiment\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# Select the first 100 rows\n",
    "first_100_rows = df[0:100]\n",
    "\n",
    "# Initialize an empty list to store the predicted sentiments\n",
    "predicted_sentiment = []\n",
    "\n",
    "# Predict the sentiment for each tweet\n",
    "for title in first_100_rows['tweet']:\n",
    "    predictions = model_name.predict([title])\n",
    "    predicted_sentiment.append(predictions[0])  # Assuming the model returns a list or array, take the first element\n",
    "\n",
    "# Add the predicted sentiments to the dataframe\n",
    "first_100_rows['predicted_sentiment'] = predicted_sentiment\n",
    "\n",
    "# Display the dataframe\n",
    "print(first_100_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Genai",
   "language": "python",
   "name": "genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
